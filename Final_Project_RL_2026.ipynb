{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LilachPardess/final-rl-project-minigrid/blob/main/Final_Project_RL_2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21qwRKOJjPRH"
      },
      "source": [
        "# üéÆ Deep RL Final Project ‚Äî MiniGrid Environments\n",
        "\n",
        "## Overview\n",
        "In this project, you will train deep RL agents on two MiniGrid environments. You must implement the core algorithms yourself using concepts from class.\n",
        "\n",
        "**Read first:** Follow the instructions in the accompanying PDF. This notebook defines the environment APIs and where you may edit.\n",
        "\n",
        "## Learning objectives\n",
        "- Implement deep RL from scratch.\n",
        "- Design observation preprocessing and reward shaping.\n",
        "- Evaluate performance with clear metrics and plots.\n",
        "- Communicate results in a concise report.\n",
        "\n",
        "## üåç The Two Environments\n",
        "| Environment | Description | Actions | Goal |\n",
        "|-------------|-------------|---------|------|\n",
        "| **`SimpleGridEnv`** | 8√ó8 empty room | 3 (Left, Right, Forward) | Navigate to the green goal square |\n",
        "| **`KeyDoorBallEnv`** | Two rooms with locked door | 5 (Left, Right, Forward, Pickup, Toggle) | Get key ‚Üí Open door ‚Üí Pick up ball ‚Üí Reach goal |\n",
        "\n",
        "## ‚úÖ What You CAN Modify\n",
        "- **Preprocessing** ‚Äî Implement your own observation preprocessing function.\n",
        "- **Reward shaping** ‚Äî Modify rewards in each `step()` method (see `# TODO`).\n",
        "- **Observation space** ‚Äî Update `self.observation_space` to match preprocessing output.\n",
        "\n",
        "## ‚ùå What You CANNOT Modify\n",
        "- Grid layout and generation logic.\n",
        "- Action spaces or termination conditions.\n",
        "- Environment mechanics (door/key/ball rules).\n",
        "- **No external RL libraries** for the core algorithm (you may use PyTorch/TensorFlow).\n",
        "\n",
        "## üì¶ Deliverables & Submission\n",
        "**Submit:** (1) this notebook with your full code and outputs, and (2) a report formatted **as specified in the instructions PDF**.\n",
        "\n",
        "## üß™ Reproducibility checklist\n",
        "- Set random seeds for Python, NumPy, and your DL framework.\n",
        "- Document hyperparameters.\n",
        "\n",
        "## üßæ Academic integrity\n",
        "- You may discuss ideas, but code and write-up must be your own.\n",
        "- Cite any external resources (blog posts, papers, code snippets).\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXbtXcLijPRI"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZJef49pjPRI"
      },
      "source": [
        "## Installs and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T3qcykHFi15"
      },
      "source": [
        "### Installs (run once if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "2dah0RrY9Kmj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#!sudo apt-get update\n",
        "#!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install imageio\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gymnasium\n",
        "!pip install minigrid\n",
        "!pip install pygame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHbKbI7BwIwv"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "T9RBKvR_jPRJ"
      },
      "outputs": [],
      "source": [
        "# 1. Future Imports (Must be first)\n",
        "from __future__ import annotations\n",
        "\n",
        "# 2. Standard Library Imports\n",
        "import base64\n",
        "import copy\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# 3. Third-Party Data & Visualization Imports\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import IPython\n",
        "from IPython.display import HTML\n",
        "import pyvirtualdisplay\n",
        "\n",
        "# 4. Gymnasium Imports\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "# 5. Minigrid Imports\n",
        "from minigrid.core.constants import COLOR_NAMES\n",
        "from minigrid.core.grid import Grid\n",
        "from minigrid.core.mission import MissionSpace\n",
        "from minigrid.core.world_object import Door, Goal, Key, Lava, Wall, Ball\n",
        "from minigrid.minigrid_env import MiniGridEnv as BaseMiniGridEnv\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# Configure Matplotlib for Notebook Environment\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7bJeRHbwMIj"
      },
      "source": [
        "### Display utils\n",
        "The cell below contains the video display configuration. No need to make changes here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "z41WGwQt9i7_"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "\n",
        "# Xvfb is Linux-only (e.g. Colab). On macOS, skip virtual display.\n",
        "import sys\n",
        "if sys.platform == \"linux\":\n",
        "    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "else:\n",
        "    display = None  # macOS: use default display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aofVkAcHzG9h"
      },
      "source": [
        "# Environments\n",
        "\n",
        "> ‚ö†Ô∏è **Important**\n",
        ">\n",
        "> The two environments below are **fixed**‚Äîdo not modify grid layout, action spaces, or termination conditions.\n",
        ">\n",
        "> **You may modify only:**\n",
        "> 1. **Observation space** ‚Äî set `self.observation_space` to match your preprocessing output (see `# TODO`).\n",
        "> 2. **Reward shaping** ‚Äî edit reward logic inside `step()` (see `# TODO`).\n",
        ">\n",
        "> Look for the clearly marked sections in each environment class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "c3u5jj6DjPRJ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENVIRONMENT 1: SIMPLE GRID (Empty Room)\n",
        "# =============================================================================\n",
        "class SimpleGridEnv(BaseMiniGridEnv):\n",
        "    \"\"\"\n",
        "    A simple grid environment with no obstacles inside, just outer walls.\n",
        "    The agent and goal positions are randomized at each reset.\n",
        "\n",
        "    Actions:\n",
        "        0: Turn Left\n",
        "        1: Turn Right\n",
        "        2: Move Forward\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        size=10,\n",
        "        max_steps=1000,\n",
        "        render_mode=\"rgb_array\",\n",
        "        preprocess=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "\n",
        "        # Define a static mission string (not used by the agent typically)\n",
        "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
        "\n",
        "\n",
        "        super().__init__(\n",
        "            mission_space=mission_space,\n",
        "            grid_size=size,\n",
        "            see_through_walls=True,\n",
        "            max_steps=max_steps,\n",
        "            render_mode=render_mode,\n",
        "            highlight=False,\n",
        "            **kwargs,\n",
        "        )\n",
        "        # Discrete actions: 0: Turn Left, 1: Turn Right, 2: Move Forward\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "        # ‚ïë  ‚úÖ STUDENT TODO: Update observation_space to match preprocessing   ‚ïë\n",
        "        # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            shape=(84, 84, 3),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "        # ‚ïë                     END OF EDITABLE SECTION                         ‚ïë\n",
        "        # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "        # Preprocessing function for observations\n",
        "        self.preprocess = preprocess if preprocess is not None else lambda x: x\n",
        "        self.walls_init = []\n",
        "        # Default goal and agent starting position (overwritten in _gen_grid)\n",
        "        self.goal_pos = (self.width - 2, self.height - 2)\n",
        "        self.agent_start_pos = (1, 1)\n",
        "        self.agent_start_dir = 0\n",
        "\n",
        "\n",
        "    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "    # ‚ïë  ‚õî DO NOT MODIFY: Core environment methods below                       ‚ïë\n",
        "    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "    @staticmethod\n",
        "    def _gen_mission():\n",
        "        return \"grand mission\"\n",
        "\n",
        "    def _get_obs(self, obs=None):\n",
        "        \"\"\"Returns the current observation after applying preprocessing.\"\"\"\n",
        "        obs = self.get_frame(highlight=False, tile_size=32)\n",
        "        return self.preprocess(obs)\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        # 1. Randomize agent starting parameters before generating the grid\n",
        "        self.agent_start_pos = (random.randint(1, 6), random.randint(1, 6))\n",
        "        self.agent_start_dir = random.choice([0, 1, 2, 3])\n",
        "        # 2. Call parent reset, which internally calls _gen_grid()\n",
        "        obs, info = super().reset(seed=seed, options=options)\n",
        "        # 3. Return the preprocessed observation\n",
        "        return self._get_obs(obs), info\n",
        "\n",
        "    def _gen_grid(self, width, height):\n",
        "        \"\"\"Procedurally generates the grid layout: walls, goal, and agent.\"\"\"\n",
        "        self.grid = Grid(width, height)\n",
        "\n",
        "        # 1. Create the outer boundary walls\n",
        "        self.grid.wall_rect(0, 0, width, height)\n",
        "\n",
        "        # 2. Place internal walls (if any defined in self.walls_init)\n",
        "        for column, row in self.walls_init:\n",
        "            self.grid.set(column, row, Wall())\n",
        "\n",
        "        # 3. Randomize Goal Position (Top-Right, Bottom-Right, Bottom-Left corners roughly)\n",
        "        self.goal_pos = random.choice([(8,1), (8,8), (1,8)])\n",
        "        self.put_obj(Goal(), self.goal_pos[0], self.goal_pos[1])\n",
        "\n",
        "        # 4. Place Agent\n",
        "        self.agent_pos = self.agent_start_pos\n",
        "        self.agent_dir = self.agent_start_dir\n",
        "\n",
        "        self.mission = \"grand mission\"\n",
        "\n",
        "    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "    # ‚ïë  ‚úÖ STUDENT TODO: Modify reward shaping below                           ‚ïë\n",
        "    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Standard step function.\n",
        "\n",
        "        Current reward scheme (sparse):\n",
        "            - +1.0 when reaching the goal (terminated=True)\n",
        "            - 0.0 otherwise\n",
        "\n",
        "        üí° You can add reward shaping here\n",
        "        \"\"\"\n",
        "        obs, reward, terminated, truncated, info = super().step(action)\n",
        "\n",
        "        # ----- REWARD SHAPING: EDIT BELOW THIS LINE -----\n",
        "        if terminated:\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = 0.0\n",
        "        # ----- REWARD SHAPING: EDIT ABOVE THIS LINE -----\n",
        "\n",
        "        return self._get_obs(obs), reward, terminated, truncated, info\n",
        "    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "    # ‚ïë                     END OF EDITABLE SECTION                             ‚ïë\n",
        "    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "e_IdnkEzjPRJ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENVIRONMENT 2: KEY-DOOR WITH BALL PICKUP\n",
        "# =============================================================================\n",
        "class KeyDoorBallEnv(BaseMiniGridEnv):\n",
        "    \"\"\"\n",
        "    Grid environment with two rooms separated by a locked door.\n",
        "\n",
        "    Task sequence:\n",
        "        1. Pick up key -> 2. Unlock door -> 3. Pick up ball -> 4. Reach goal\n",
        "\n",
        "    Actions:\n",
        "        0: Turn Left\n",
        "        1: Turn Right\n",
        "        2: Move Forward\n",
        "        3: Pick Up\n",
        "        4: Toggle (open/close door)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        size=10,\n",
        "        max_steps=1000,\n",
        "        render_mode=\"rgb_array\",\n",
        "        partition_col=3,\n",
        "        require_ball_pickup=True,\n",
        "        preprocess=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.agent_start_pos = (1, 1)\n",
        "        self.agent_start_dir = 0\n",
        "        self.partition_col = partition_col\n",
        "        self.walls_init = []\n",
        "        self.inventory = []\n",
        "        self.require_ball_pickup = require_ball_pickup\n",
        "        self.preprocess = preprocess if preprocess is not None else lambda x: x\n",
        "\n",
        "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
        "        super().__init__(\n",
        "            mission_space=mission_space,\n",
        "            grid_size=size,\n",
        "            see_through_walls=True,\n",
        "            max_steps=max_steps,\n",
        "            render_mode=render_mode,\n",
        "            highlight=False,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        # 5 actions: left, right, forward, pickup, toggle\n",
        "        self.action_space = spaces.Discrete(5)\n",
        "\n",
        "        # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "        # ‚ïë  ‚úÖ STUDENT TODO: Update observation_space to match preprocessing   ‚ïë\n",
        "        # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            shape=(84, 84, 3),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "        # ‚ïë                     END OF EDITABLE SECTION                         ‚ïë\n",
        "        # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "        # State tracking for reward shaping (you can use these in your reward logic)\n",
        "        self.prev_key = False\n",
        "        self.prev_door = False\n",
        "        self.prev_ball = False\n",
        "\n",
        "    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "    # ‚ïë  ‚õî DO NOT MODIFY: Core environment methods below                       ‚ïë\n",
        "    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "    @staticmethod\n",
        "    def _gen_mission():\n",
        "        return \"Pick up the key to open the door, pick up the ball, then reach the goal\"\n",
        "\n",
        "    def _get_obs(self, obs=None):\n",
        "        \"\"\"Returns the current observation after applying preprocessing.\"\"\"\n",
        "        obs = self.get_frame(highlight=False, tile_size=32)\n",
        "        return self.preprocess(obs)\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        # Reset state tracking\n",
        "        self.prev_key = False\n",
        "        self.prev_door = False\n",
        "        self.prev_ball = False\n",
        "        self.inventory = []\n",
        "\n",
        "        # Call parent reset, which internally calls _gen_grid()\n",
        "        obs, info = super().reset(seed=seed, options=options)\n",
        "\n",
        "        return self._get_obs(obs), info\n",
        "\n",
        "    def _gen_grid(self, width, height):\n",
        "        \"\"\"Generate grid: walls, partition, door, key, ball, goal, agent.\"\"\"\n",
        "        # Grid with outer walls\n",
        "        self.grid = Grid(width, height)\n",
        "        self.grid.wall_rect(0, 0, width, height)\n",
        "\n",
        "        # Partition wall\n",
        "        self.walls_init = [(self.partition_col, i) for i in range(height)]\n",
        "        for col, row in self.walls_init:\n",
        "            if 0 <= col < width and 0 <= row < height:\n",
        "                self.grid.set(col, row, Wall())\n",
        "\n",
        "        # Key in left room\n",
        "        self.key_pos = (\n",
        "            random.choice(range(1, self.partition_col)),\n",
        "            random.choice(range(2, height - 1))\n",
        "        )\n",
        "        self.grid.set(self.key_pos[0], self.key_pos[1], Key(COLOR_NAMES[0]))\n",
        "\n",
        "        # Door in partition\n",
        "        door_y = random.choice(range(1, height - 1))\n",
        "        self.door_pos = (self.partition_col, door_y)\n",
        "        self.env_door = Door(COLOR_NAMES[0], is_locked=True)\n",
        "        self.grid.set(self.door_pos[0], self.door_pos[1], self.env_door)\n",
        "\n",
        "        # Goal\n",
        "        self.goal_pos = (8, 8)\n",
        "        self.put_obj(Goal(), self.goal_pos[0], self.goal_pos[1])\n",
        "\n",
        "        # Ball in right room (if required)\n",
        "        if self.require_ball_pickup:\n",
        "            right_x = range(self.partition_col + 2, width - 2)\n",
        "            right_y = range(1, height - 1)\n",
        "            while True:\n",
        "                ball_x = random.choice(list(right_x))\n",
        "                ball_y = random.choice(list(right_y))\n",
        "                self.ball_pos = (ball_x, ball_y)\n",
        "                if self.ball_pos != self.goal_pos:\n",
        "                    break\n",
        "\n",
        "            self.grid.set(ball_x, ball_y, Ball(COLOR_NAMES[1]))\n",
        "\n",
        "        # Agent\n",
        "        self.agent_pos = self.agent_start_pos\n",
        "        self.agent_dir = self.agent_start_dir\n",
        "\n",
        "    def try_pickup_ball(self):\n",
        "        \"\"\"Pick up ball in front and add to inventory.\"\"\"\n",
        "        obj = self.grid.get(self.front_pos[0], self.front_pos[1])\n",
        "        if isinstance(obj, Ball):\n",
        "            self.grid.set(self.front_pos[0], self.front_pos[1], None)\n",
        "            self.inventory.append(obj)\n",
        "\n",
        "    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "    # ‚ïë  ‚úÖ STUDENT TODO: Modify reward shaping below                           ‚ïë\n",
        "    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Step function with sparse reward.\n",
        "\n",
        "        Available helper methods for reward shaping:\n",
        "            - self.is_carrying_key()  : Returns True if agent has the key\n",
        "            - self.is_carrying_ball() : Returns True if agent has the ball\n",
        "            - self.is_door_open()     : Returns True if door is open\n",
        "            - self.prev_key           : Key status before this step\n",
        "            - self.prev_door          : Door status before this step\n",
        "            - self.prev_ball          : Ball status before this step\n",
        "\n",
        "        Current reward scheme (sparse):\n",
        "            - +1.0 for reaching goal with ball\n",
        "            - 0.0 otherwise\n",
        "\n",
        "        üí° You can add reward shaping here\n",
        "        \"\"\"\n",
        "        # Map action 4 to toggle (internal MiniGrid uses 5 for toggle)\n",
        "        if action == 4:\n",
        "            action = 5\n",
        "\n",
        "        # Track previous state for reward shaping\n",
        "        self.prev_key = self.is_carrying_key()\n",
        "        self.prev_door = self.is_door_open()\n",
        "        self.prev_ball = self.is_carrying_ball()\n",
        "\n",
        "        # Handle ball pickup\n",
        "        if action == 3:\n",
        "            self.try_pickup_ball()\n",
        "\n",
        "        # Standard step\n",
        "        obs, reward, terminated, truncated, info = super().step(action)\n",
        "\n",
        "        # Goal only counts if ball is picked up (when required)\n",
        "        terminated = terminated and (not self.require_ball_pickup or self.is_carrying_ball())\n",
        "\n",
        "        # ----- REWARD SHAPING: EDIT BELOW THIS LINE -----\n",
        "        if terminated:\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = 0.0\n",
        "        # ----- REWARD SHAPING: EDIT ABOVE THIS LINE -----\n",
        "\n",
        "        return self._get_obs(obs), reward, terminated, truncated, info\n",
        "    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "    # ‚ïë                     END OF EDITABLE SECTION                             ‚ïë\n",
        "    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "    # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "    # ‚ïë  ‚õî DO NOT MODIFY: State getter methods (use these in reward shaping)   ‚ïë\n",
        "    # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "    def is_carrying_key(self):\n",
        "        \"\"\"Check if agent has key (in hand or inventory).\"\"\"\n",
        "        key_in_hand = self.carrying and isinstance(self.carrying, Key)\n",
        "        key_in_inventory = any(isinstance(item, Key) for item in self.inventory)\n",
        "        return key_in_hand or key_in_inventory\n",
        "\n",
        "    def is_carrying_ball(self):\n",
        "        \"\"\"Check if agent has ball (in hand or inventory).\"\"\"\n",
        "        ball_in_hand = self.carrying and isinstance(self.carrying, Ball)\n",
        "        ball_in_inventory = any(isinstance(item, Ball) for item in self.inventory)\n",
        "        return ball_in_hand or ball_in_inventory\n",
        "\n",
        "    def is_door_open(self):\n",
        "        \"\"\"Returns True if the door is open.\"\"\"\n",
        "        if hasattr(self, 'env_door'):\n",
        "            return self.env_door.is_open\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MHBOpXqjPRJ"
      },
      "source": [
        "# Preprocessing Method\n",
        "\n",
        "> üí° **Student task:** Implement your own preprocessing function below.\n",
        ">\n",
        "> Your preprocessing function should:\n",
        "> 1. Take a raw RGB image (320√ó320√ó3) as input.\n",
        "> 2. Return a processed observation that **exactly matches** your `observation_space` (shape, dtype, value range).\n",
        ">\n",
        "> Common preprocessing techniques:\n",
        "> - Crop edges to remove borders\n",
        "> - Convert to grayscale\n",
        "> - Resize to smaller dimensions\n",
        "> - Normalize pixel values\n",
        ">\n",
        "> **Reminder:** Update `self.observation_space` in both environment classes to match your output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "WvzHckRdjPRJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 26.0 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "# Ensure opencv-python is installed in the current kernel (run once, then re-run cell if import fails)\n",
        "import subprocess, sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\", \"-q\"])\n",
        "\n",
        "import cv2\n",
        "\n",
        "# Example Preprocessing Function\n",
        "PREPROCESS_SIZE = (84, 84)  # (height, width) ‚Äî smaller = faster training, less memory\n",
        "\n",
        "def pre_process(img):\n",
        "    \"\"\"\n",
        "    Preprocess raw RGB observation from the environment.\n",
        "    1. Copy to array\n",
        "    2. Resize to smaller dimensions (e.g. 84x84) for faster training and less replay memory.\n",
        "    3. Normalize pixel values to [0, 1] (divide by 255) for stable neural network training.\n",
        "\n",
        "    Input:  RGB image (320, 320, 3), uint8 [0, 255]\n",
        "    Output: RGB image (84, 84, 3), float32 [0.0, 1.0]\n",
        "    \"\"\"\n",
        "    arr = np.array(img, copy=True)\n",
        "    resized = cv2.resize(arr, (PREPROCESS_SIZE[1], PREPROCESS_SIZE[0]), interpolation=cv2.INTER_AREA)\n",
        "    normalized = resized.astype(np.float32) / 255.0\n",
        "\n",
        "    #TODO check if check use several images\n",
        "    return normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDgIa_NOjPRJ"
      },
      "source": [
        "# üîç Environment Examples & Exploration\n",
        "Run the cells below to understand each environment before implementing your solution.\n",
        "Use these quick tests to verify preprocessing output shape and rendering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWlbg9r96_08"
      },
      "source": [
        "## Environment 1: SimpleGridEnv (Empty Room)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E22QYJKSyEsF"
      },
      "source": [
        "**Task**: Navigate to the green goal square in an empty 8√ó8 room.\n",
        "\n",
        "| Property | Value |\n",
        "|----------|-------|\n",
        "| Grid Size | 8√ó8 |\n",
        "| Agent Start | Random position (1-6, 1-6) |\n",
        "| Agent Direction | Random (0-3) |\n",
        "| Goal Position | Random: (8,1), (1,8), or (8,8) |\n",
        "| Actions | 0: Turn Left, 1: Turn Right, 2: Move Forward |\n",
        "| Sparse Reward | +1.0 on reaching goal, 0.0 otherwise |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSe2m1kWwQ5L"
      },
      "source": [
        "### Quick Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "bGwG4ekDoPB-",
        "outputId": "f7e195af-bd9c-489a-b4e3-248a1f3d527b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SimpleGridEnv ===\n",
            "Action space:       Discrete(3)\n",
            "Number of actions:  3\n",
            "Observation space:  Box(0.0, 1.0, (84, 84, 3), float32)  (ensure it matches preprocessing output)\n",
            "Observation shape:  (84, 84, 3)\n",
            "Agent direction:    0\n",
            "Agent position:     (3, 6)\n",
            "Goal position:      (1, 8)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize environment with preprocessing\n",
        "env = SimpleGridEnv(max_steps=100, preprocess=pre_process)\n",
        "obs = env.reset()[0]\n",
        "\n",
        "print(\"=== SimpleGridEnv ===\")\n",
        "print(f\"Action space:       {env.action_space}\")\n",
        "print(f\"Number of actions:  {env.action_space.n}\")\n",
        "print(f\"Observation space:  {env.observation_space}  (ensure it matches preprocessing output)\")\n",
        "print(f\"Observation shape:  {obs.shape}\")\n",
        "print(f\"Agent direction:    {env.agent_dir}\")\n",
        "print(f\"Agent position:     {env.agent_pos}\")\n",
        "print(f\"Goal position:      {env.goal_pos}\")\n",
        "\n",
        "# Side-by-side view of raw and preprocessed observations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "axes[0].imshow(env.render())\n",
        "axes[0].set_title(\"SimpleGridEnv (raw)\")\n",
        "axes[0].axis(\"off\")\n",
        "axes[1].imshow(obs.squeeze(), cmap=\"gray\")\n",
        "axes[1].set_title(\"Preprocessed Observation\")\n",
        "axes[1].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Qs2iJl6BCg"
      },
      "source": [
        "## Environment 2: KeyDoorBallEnv (Key-Door-Ball Task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNNOKucFjPRK"
      },
      "source": [
        "**Task**: Complete a multi-step objective: Key ‚Üí Door ‚Üí Ball ‚Üí Goal\n",
        "\n",
        "| Property | Value |\n",
        "|----------|-------|\n",
        "| Grid Size | 8√ó8 (two rooms) |\n",
        "| Partition Wall | Column 3 |\n",
        "| Key Location | Left room (random) |\n",
        "| Door Location | In partition wall (random row) |\n",
        "| Ball Location | Right room (random) |\n",
        "| Goal Position | Fixed at (8, 8) |\n",
        "| Actions | 0: Left, 1: Right, 2: Forward, 3: Pickup, 4: Toggle |\n",
        "\n",
        "**Sequence**: Pick up key ‚Üí Open door ‚Üí Enter right room ‚Üí Pick up ball ‚Üí Reach goal\n",
        "\n",
        "**Helper Methods** (use in reward shaping):\n",
        "- `is_carrying_key()` ‚Äî True if agent has the key\n",
        "- `is_carrying_ball()` ‚Äî True if agent has the ball\n",
        "- `is_door_open()` ‚Äî True if door is open"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0XAsSMBjPRK"
      },
      "source": [
        "### Quick Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "bsZpzNRe6Gen",
        "outputId": "ef92861a-5bae-42c3-a4a4-d901e46a0290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== KeyDoorBallEnv ===\n",
            "Action space:       Discrete(5)\n",
            "Number of actions:  5\n",
            "Observation space:  Box(0.0, 1.0, (84, 84, 3), float32)  (ensure it matches preprocessing output)\n",
            "Observation shape:  (84, 84, 3)\n",
            "Agent direction:    0\n",
            "Agent position:     (1, 1)\n",
            "Goal position:      (8, 8)\n",
            "Carrying key:       False\n",
            "Door open:          False\n",
            "Carrying ball:      False\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize environment\n",
        "env = KeyDoorBallEnv(max_steps=5000, preprocess=pre_process)\n",
        "obs = env.reset()[0]\n",
        "\n",
        "print(\"=== KeyDoorBallEnv ===\")\n",
        "print(f\"Action space:       {env.action_space}\")\n",
        "print(f\"Number of actions:  {env.action_space.n}\")\n",
        "print(f\"Observation space:  {env.observation_space}  (ensure it matches preprocessing output)\")\n",
        "print(f\"Observation shape:  {obs.shape}\")\n",
        "print(f\"Agent direction:    {env.agent_dir}\")\n",
        "print(f\"Agent position:     {env.agent_pos}\")\n",
        "print(f\"Goal position:      {env.goal_pos}\")\n",
        "print(f\"Carrying key:       {env.is_carrying_key()}\")\n",
        "print(f\"Door open:          {env.is_door_open()}\")\n",
        "print(f\"Carrying ball:      {env.is_carrying_ball()}\")\n",
        "\n",
        "# Side-by-side view of raw and preprocessed observations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "axes[0].imshow(env.render())\n",
        "axes[0].set_title(\"KeyDoorBallEnv (raw)\")\n",
        "axes[0].axis(\"off\")\n",
        "axes[1].imshow(obs.squeeze(), cmap=\"gray\")\n",
        "axes[1].set_title(\"Preprocessed Observation\")\n",
        "axes[1].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "7RjyZDC_jPRK",
        "outputId": "9ad92551-2fbc-4519-8406-8a3e667a0cb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Episode Summary ===\n",
            "Steps:         100\n",
            "Total reward:  0.000\n",
            "Terminated:    False\n",
            "Truncated:     True\n",
            "\n",
            "=== Environment Status (End of Episode) ===\n",
            "Agent position:  (np.int64(1), np.int64(2))\n",
            "Carrying key:    False\n",
            "Door open:       False\n",
            "Carrying ball:   False\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <video width=\"640\" height=\"480\" controls>\n",
              "    <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              "  Your browser does not support the video tag.\n",
              "  </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Random action rollout + video\n",
        "max_steps = 100\n",
        "env = KeyDoorBallEnv(max_steps=max_steps, preprocess=pre_process)\n",
        "num_actions = env.action_space.n\n",
        "obs = env.reset()[0]\n",
        "video_filename = \"KeyDoorBallEnv_random.mp4\"\n",
        "\n",
        "with imageio.get_writer(video_filename, fps=10) as video:\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    for step in range(max_steps):\n",
        "        action = random.randint(0, num_actions - 1)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "        video.append_data(env.render())\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "# Print episode summary\n",
        "print(\"=== Episode Summary ===\")\n",
        "print(f\"Steps:         {step + 1}\")\n",
        "print(f\"Total reward:  {total_reward:.3f}\")\n",
        "print(f\"Terminated:    {terminated}\")\n",
        "print(f\"Truncated:     {truncated}\")\n",
        "print()\n",
        "print(\"=== Environment Status (End of Episode) ===\")\n",
        "print(f\"Agent position:  {env.agent_pos}\")\n",
        "print(f\"Carrying key:    {env.is_carrying_key()}\")\n",
        "print(f\"Door open:       {env.is_door_open()}\")\n",
        "print(f\"Carrying ball:   {env.is_carrying_ball()}\")\n",
        "\n",
        "embed_mp4(video_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPJZuciHjPRK"
      },
      "source": [
        "# Your Code Below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "vO4qOpN1jPRK"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Select best available device: CUDA > MPS (Apple Silicon) > CPU.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda:0\")\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")  # Apple Silicon GPU\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "# ==========================================\n",
        "# Config (hyperparameters and env-derived settings)\n",
        "# ==========================================\n",
        "class Config:\n",
        "    \"\"\"Holds agent/environment config. Pass env to fill input_shape and num_actions from it.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        env=None,\n",
        "        memory_size=10_000,\n",
        "        minibatch_size=64,\n",
        "        discount_factor=0.99,\n",
        "        total_episodes=1000,\n",
        "        epsilon=1.0,\n",
        "        epsilon_ending_value=0.01,\n",
        "        epsilon_decay_value=0.995,\n",
        "        frame_skipping=1,\n",
        "        **kwargs\n",
        "    ):\n",
        "        obs_space = env.observation_space\n",
        "        raw_shape = getattr(obs_space, \"shape\", (3, 84, 84))\n",
        "        # PyTorch Conv2d expects (C, H, W). If env gives (H, W, C), convert.\n",
        "        if len(raw_shape) == 3 and raw_shape[-1] in (1, 3):\n",
        "            self.input_shape = (raw_shape[-1], raw_shape[0], raw_shape[1])\n",
        "        else:\n",
        "            self.input_shape = raw_shape\n",
        "            self.num_actions = env.action_space.n\n",
        "        self.memory_size = memory_size\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.discount_factor = discount_factor\n",
        "        # Training loop hyperparameters\n",
        "        self.total_episodes = total_episodes\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_ending_value = epsilon_ending_value\n",
        "        self.epsilon_decay_value = epsilon_decay_value\n",
        "        self.frame_skipping = frame_skipping\n",
        "        self.num_actions = env.action_space.n\n",
        "\n",
        "# ==========================================\n",
        "# The Model\n",
        "# ==========================================\n",
        "class MiniGridNet(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions, mode='value'):\n",
        "        super().__init__()\n",
        "        # input_shape must be (C, H, W) for Conv2d\n",
        "        c, h, w = input_shape[0], input_shape[1], input_shape[2]\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=16, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        # Compute conv output size so it works for any input (C, H, W)\n",
        "        def _conv_out(in_size, k, s):\n",
        "            return (in_size - k) // s + 1\n",
        "        h1, w1 = _conv_out(h, 8, 4), _conv_out(w, 8, 4)\n",
        "        h2, w2 = _conv_out(h1, 4, 2), _conv_out(w1, 4, 2)\n",
        "        flat_size = 32 * h2 * w2\n",
        "\n",
        "        self.q_head = nn.Sequential(\n",
        "            nn.Linear(flat_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        return self.q_head(features)\n",
        "\n",
        "# ==========================================\n",
        "# THE MEMORY\n",
        "# ==========================================\n",
        "class ExperienceMemory:\n",
        "    def __init__(self, capacity, is_on_policy=False):\n",
        "        # Use GPU if available (CUDA or MPS for Apple Silicon), otherwise CPU\n",
        "        self.device = get_device()\n",
        "        # Set the maximum memory capacity\n",
        "        self.capacity = capacity\n",
        "        # Initialize the memory as a list\n",
        "        self.memory = []\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Randomly sample a batch of experiences\n",
        "        experiences = random.sample(self.memory, k=batch_size)\n",
        "\n",
        "        # Extract each part of the experience tuple (state, action, reward, next_state, done)\n",
        "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
        "\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        return states, next_states, actions, rewards, dones\n",
        "\n",
        "    def push(self, experience):\n",
        "        # Add the new experience to memory\n",
        "        self.memory.append(experience)\n",
        "\n",
        "        # If memory exceeds the capacity, remove the oldest experience\n",
        "        if len(self.memory) > self.capacity:\n",
        "            self.memory.pop(0)  # Remove the first item (FIFO - First In, First Out)\n",
        "    \n",
        "    \n",
        "# ==========================================\n",
        "# THE AGENT (The Logic)\n",
        "# ==========================================\n",
        "class Agent:\n",
        "    def __init__(self, config):\n",
        "        self.device = get_device()  # CUDA GPU > MPS (Apple Silicon) > CPU\n",
        "        self.network = MiniGridNet(config.input_shape, config.num_actions).to(self.device)\n",
        "        self.target_network = copy.deepcopy(self.network)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=1e-4)\n",
        "        self.memory = ExperienceMemory(config.memory_size)\n",
        "        self.minibatch_size = config.minibatch_size\n",
        "        self.num_actions = config.num_actions\n",
        "\n",
        "    def _to_chw(self, x):\n",
        "        \"\"\"Convert (N, H, W, C) or (H, W, C) to (N, C, H, W) for Conv2d.\"\"\"\n",
        "        if x.dim() == 3 and x.shape[-1] in (1, 3):\n",
        "            x = x.permute(2, 0, 1).unsqueeze(0)\n",
        "        elif x.dim() == 4 and x.shape[-1] in (1, 3):\n",
        "            x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        return x\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, self.num_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            if not torch.is_tensor(state):\n",
        "                state = torch.tensor(state, dtype=torch.float32)\n",
        "            state = state.to(self.device)\n",
        "            state = self._to_chw(state)\n",
        "            q_values = self.network(state)\n",
        "            return q_values.argmax(dim=1).item()\n",
        "\n",
        "    def train_step(self, config):\n",
        "        states, next_states, actions, rewards, dones = self.memory.sample(self.minibatch_size)\n",
        "        states = self._to_chw(states)\n",
        "        next_states = self._to_chw(next_states)\n",
        "        actions = actions.view(-1, 1)\n",
        "\n",
        "        next_q_targets = self.target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        q_targets = rewards + (config.discount_factor * next_q_targets * (1 - dones))\n",
        "        q_expected = self.network(states).gather(1, actions.to(self.device))\n",
        "        loss = F.mse_loss(q_expected, q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        #TODO update target network\n",
        "        #TODO undestand the different between the local and the target network\n",
        "\n",
        "    def update_target_network(self):\n",
        "        for target_param, local_param in zip(self.target_network.parameters(), self.network.parameters()):\n",
        "            target_param.data.copy_(local_param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on: mps\n",
            "GPU: Apple Silicon (MPS)\n"
          ]
        }
      ],
      "source": [
        "# Run this cell anytime to see if you're on CPU or GPU (run after the cell that defines get_device)\n",
        "device = get_device()\n",
        "print(\"Running on:\", device)\n",
        "if device.type == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "elif device.type == \"mps\":\n",
        "    print(\"GPU: Apple Silicon (MPS)\")\n",
        "else:\n",
        "    print(\"GPU: None (CPU only)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### plot utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_training_setup(env, config, agent):\n",
        "    \"\"\"Print important config, agent, and network parameters before training.\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"CONFIG\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"  env:                  {env.spec.id if env.spec else type(env).__name__}\")\n",
        "    print(f\"  observation shape:   {config.input_shape} (C, H, W)\")\n",
        "    print(f\"  num_actions:         {config.num_actions}\")\n",
        "    print(f\"  memory_size:         {config.memory_size}\")\n",
        "    print(f\"  minibatch_size:      {config.minibatch_size}\")\n",
        "    print(f\"  discount_factor:     {config.discount_factor}\")\n",
        "    print(f\"  total_episodes:      {config.total_episodes}\")\n",
        "    print(f\"  epsilon:             {config.epsilon} -> {config.epsilon_ending_value} (decay {config.epsilon_decay_value})\")\n",
        "    print(f\"  frame_skipping:      {config.frame_skipping}\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"AGENT / NETWORK\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"  device:              {agent.device}\")\n",
        "    n_params = sum(p.numel() for p in agent.network.parameters())\n",
        "    print(f\"  network parameters:  {n_params:,}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# Window size for rolling average (last N episodes)\n",
        "LAST_EPISODES = 300\n",
        "scores_on_last_windowSize_episodes = deque(maxlen=LAST_EPISODES)\n",
        "\n",
        "\n",
        "def plot_training_progress(scores, window_size=50, solving_threshold=None):\n",
        "    \"\"\"\n",
        "    Plot the training progress of a DQN agent.\n",
        "\n",
        "    Args:\n",
        "        scores (list): List of scores from each episode\n",
        "        window_size (int): Size of the moving average window\n",
        "        solving_threshold (float, optional): If set, draw a horizontal line at this score.\n",
        "                                             For MiniGrid use e.g. 1.0 or None to omit.\n",
        "    \"\"\"\n",
        "    scores = np.array(scores)\n",
        "\n",
        "    moving_averages = []\n",
        "    for i in range(len(scores)):\n",
        "        if i < window_size:\n",
        "            moving_averages.append(np.mean(scores[:i+1]))\n",
        "        else:\n",
        "            moving_averages.append(np.mean(scores[i-window_size+1:i+1]))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(scores, label='Score', alpha=0.3, color='blue')\n",
        "    plt.plot(moving_averages, label=f'{window_size}-episode Moving Average',\n",
        "             color='red', linewidth=2)\n",
        "\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('DQN Training Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    if solving_threshold is not None:\n",
        "        plt.axhline(y=solving_threshold, color='green', linestyle='--', alpha=0.5,\n",
        "                    label='Solving Threshold')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "#  MAIN TRAINING LOOP (Static Method)\n",
        "# ==========================================\n",
        "def run_training(env, agent, config, preprocess_observation, print_every=100,\n",
        "                 solving_threshold=None, checkpoint_path='checkpoint.pth'):\n",
        "    \"\"\"\n",
        "    Run the main training loop. Returns scores_history for plotting.\n",
        "    Hyperparameters (total_episodes, epsilon, frame_skipping, etc.) are read from config.\n",
        "    \"\"\"\n",
        "    total_steps = 0\n",
        "    epsilon = config.epsilon\n",
        "    scores_history = []\n",
        "    scores_window = deque(maxlen=LAST_EPISODES)\n",
        "    print(f\"Starting training: {config.total_episodes} episodes, epsilon={epsilon:.3f} -> {config.epsilon_ending_value:.3f}\")\n",
        "    print(\"-\" * 60)\n",
        "    for episode in range(config.total_episodes):\n",
        "        obs, info = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        episode_steps = 0\n",
        "        while not done:  # TODO check if max steps is used or not\n",
        "            # TODO check if preprocess is needed or is it already done in the step function\n",
        "            state = preprocess_observation(obs)\n",
        "            action = agent.select_action(state, epsilon)\n",
        "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "            episode_steps += 1\n",
        "\n",
        "            # TODO handle score history for plotting (per-step if needed)\n",
        "\n",
        "            next_state = preprocess_observation(next_obs)\n",
        "            # TODO handle memory size\n",
        "            agent.memory.push((state, action, reward, next_state, done))\n",
        "\n",
        "            epsilon = max(config.epsilon_ending_value, config.epsilon_decay_value * epsilon)\n",
        "\n",
        "            if total_steps % config.frame_skipping == 0:\n",
        "                agent.train_step(config)\n",
        "\n",
        "            total_steps += 1\n",
        "            # TODO check if need to update target network\n",
        "            obs = next_obs\n",
        "\n",
        "        scores_history.append(episode_reward)\n",
        "        scores_window.append(episode_reward)\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(\n",
        "            episode + 1, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "        if (episode + 1) % print_every == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.4f}'.format(\n",
        "                episode + 1, np.mean(scores_window), epsilon))\n",
        "\n",
        "        if solving_threshold is not None and len(scores_window) == LAST_EPISODES:\n",
        "            if np.mean(scores_window) >= solving_threshold:\n",
        "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(\n",
        "                    episode + 1 - LAST_EPISODES, np.mean(scores_window)))\n",
        "                torch.save(agent.network.state_dict(), checkpoint_path)\n",
        "                break\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(\"Training finished.\")\n",
        "    return scores_history\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "CONFIG\n",
            "==================================================\n",
            "  env:                  KeyDoorBallEnv\n",
            "  observation shape:   (3, 84, 84) (C, H, W)\n",
            "  num_actions:         5\n",
            "  memory_size:         1\n",
            "  minibatch_size:      1\n",
            "  discount_factor:     0.99\n",
            "  total_episodes:      1000\n",
            "  epsilon:             1.0 -> 0.01 (decay 0.995)\n",
            "  frame_skipping:      1\n",
            "==================================================\n",
            "AGENT / NETWORK\n",
            "==================================================\n",
            "  device:              mps\n",
            "  network parameters:  1,341,493\n",
            "==================================================\n",
            "Starting training: 1000 episodes, epsilon=1.000 -> 0.010\n",
            "------------------------------------------------------------\n",
            "Episode 30\tAverage Score: 0.00"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[89], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m print_training_setup(env, config, agent)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Run training and collect scores for plotting (solving_threshold=None for MiniGrid; set e.g. 1.0 to stop early)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m scores_history \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_observation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msolving_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Plot training progress (solving_threshold=None omits the green line; set e.g. 200 for CartPole-style)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m plot_training_progress(scores_history, window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, solving_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[88], line 23\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(env, agent, config, preprocess_observation, print_every, solving_threshold, checkpoint_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     22\u001b[0m     state \u001b[38;5;241m=\u001b[39m preprocess_observation(obs)\n\u001b[0;32m---> 23\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     25\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
            "Cell \u001b[0;32mIn[85], line 150\u001b[0m, in \u001b[0;36mAgent.select_action\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(state):\n\u001b[1;32m    149\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 150\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_chw(state)\n\u001b[1;32m    152\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(state)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# run the training loop (hyperparameters passed via config)\n",
        "config = Config(\n",
        "    env=env,\n",
        "    memory_size=1,\n",
        "    minibatch_size=1,\n",
        "    discount_factor=0.99,\n",
        "    total_episodes=1000,\n",
        "    epsilon=1.0,\n",
        "    epsilon_ending_value=0.01,\n",
        "    epsilon_decay_value=0.995,\n",
        "    frame_skipping=1\n",
        ")\n",
        "preprocess_observation = pre_process\n",
        "agent = Agent(config)\n",
        "\n",
        "print_training_setup(env, config, agent)\n",
        "\n",
        "# Run training and collect scores for plotting (solving_threshold=None for MiniGrid; set e.g. 1.0 to stop early)\n",
        "scores_history = run_training(env, agent, config, preprocess_observation, print_every=100,\n",
        "                               solving_threshold=None, checkpoint_path='checkpoint.pth')\n",
        "\n",
        "# Plot training progress (solving_threshold=None omits the green line; set e.g. 200 for CartPole-style)\n",
        "plot_training_progress(scores_history, window_size=50, solving_threshold=None)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RXbtXcLijPRI",
        "8MHBOpXqjPRJ"
      ],
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
